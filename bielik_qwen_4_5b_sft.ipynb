{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install libs"
      ],
      "metadata": {
        "id": "IDTpNRGyXVq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "s3yO6LObXUu1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drive for train and val datasets"
      ],
      "metadata": {
        "id": "j_hoYlkiVB2j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmmaeG0iUIr5",
        "outputId": "58f33386-da07-4c88-8cc0-b3f08df82486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Config"
      ],
      "metadata": {
        "id": "XzbJFBawVAZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_path = \"drive/MyDrive/mipd_test.jsonl\"\n",
        "GRAD_ACCUM = 8\n",
        "MAX_NEW_TOKENS = 256\n",
        "BATCH_SIZE = 1"
      ],
      "metadata": {
        "id": "HIcv7HdDuN2g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load model"
      ],
      "metadata": {
        "id": "srPezzz9VO1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "\n",
        "\n",
        "base_model_dir = \"drive/MyDrive/bielik-4.5b-base\"\n",
        "\n",
        "max_seq_length = 32768\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model_dir,\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV8d5cZcW4P7",
        "outputId": "55290852-1a2d-4c4f-ef5a-3f5601449a6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Add LoRA adapters"
      ],
      "metadata": {
        "id": "zRfC62B2VFQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I33-98WrVH-R",
        "outputId": "b50bae4c-4f04-494f-dc2a-ffa60a0438c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
            "are not enabled or a bias term (like in Qwen) is used.\n",
            "Not an error, but Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\n",
            "are not enabled or a bias term (like in Qwen) is used.\n",
            "Not an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\n",
            "are not enabled or a bias term (like in Qwen) is used.\n",
            "Unsloth 2026.1.2 patched 60 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load datasets"
      ],
      "metadata": {
        "id": "cs4K43McVMB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_data_path = \"drive/MyDrive/mipd_train.jsonl\"\n",
        "val_data_path = \"drive/MyDrive/mipd_val.jsonl\"\n",
        "\n",
        "# Load the datasets\n",
        "datasets = load_dataset(\"json\", data_files={\n",
        "    \"train\": train_data_path,\n",
        "    \"validation\": val_data_path\n",
        "}, streaming=True)\n",
        "\n",
        "print(datasets)\n",
        "\n",
        "train = datasets[\"train\"]\n",
        "val = datasets[\"validation\"]"
      ],
      "metadata": {
        "id": "i5hBwI1qVbW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e6015c-4d15-430d-c975-5843a9fade6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IterableDatasetDict({\n",
            "    train: IterableDataset({\n",
            "        features: Unknown,\n",
            "        num_shards: 1\n",
            "    })\n",
            "    validation: IterableDataset({\n",
            "        features: Unknown,\n",
            "        num_shards: 1\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[101:103]"
      ],
      "metadata": {
        "id": "WlDLA1tmgWTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74bdd9ca-312a-4de7-da5f-4dab3d5df127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': ['Jeste≈õ ekspertem ds. weryfikacji informacji i analizy medi√≥w. Twoim zadaniem jest przeanalizowanie podanego artyku≈Çu i zidentyfikowanie w nim technik manipulacji.\\n\\nDozwolone kategorie manipulacji to:\\n- REFERENCE_ERROR\\n- WHATABOUTISM\\n- STRAWMAN\\n- EMOTIONAL_CONTENT\\n- CHERRY_PICKING\\n- FALSE_CAUSE\\n- MISLEADING_CLICKBAIT\\n- ANECDOTE\\n- LEADING_QUESTIONS\\n- EXAGGERATION\\n- QUOTE_MINING\\n\\nInstrukcja:\\n1. Przeanalizuj dok≈Çadnie tre≈õƒá podanego artyku≈Çu.\\n2. Dopasuj fragmenty tekstu do powy≈ºszych definicji.\\n3. Zwr√≥ƒá wynik WY≈ÅƒÑCZNIE w formacie JSON z kluczem \"discovered_techniques\".\\n4. Je≈õli nie wykryto ≈ºadnych technik, zwr√≥ƒá pustƒÖ listƒô.\\n5. Nie generuj ≈ºadnych dodatkowych wyja≈õnie≈Ñ, wstƒôp√≥w ani komentarzy.\\n\\nPrzyk≈Çad wyj≈õcia:\\n{\"discovered_techniques\": [\"EMOTIONAL_CONTENT\", \"EXAGGERATION\"]}',\n",
              "  'Jeste≈õ ekspertem ds. weryfikacji informacji i analizy medi√≥w. Twoim zadaniem jest przeanalizowanie podanego artyku≈Çu i zidentyfikowanie w nim technik manipulacji.\\n\\nDozwolone kategorie manipulacji to:\\n- REFERENCE_ERROR\\n- WHATABOUTISM\\n- STRAWMAN\\n- EMOTIONAL_CONTENT\\n- CHERRY_PICKING\\n- FALSE_CAUSE\\n- MISLEADING_CLICKBAIT\\n- ANECDOTE\\n- LEADING_QUESTIONS\\n- EXAGGERATION\\n- QUOTE_MINING\\n\\nInstrukcja:\\n1. Przeanalizuj dok≈Çadnie tre≈õƒá podanego artyku≈Çu.\\n2. Dopasuj fragmenty tekstu do powy≈ºszych definicji.\\n3. Zwr√≥ƒá wynik WY≈ÅƒÑCZNIE w formacie JSON z kluczem \"discovered_techniques\".\\n4. Je≈õli nie wykryto ≈ºadnych technik, zwr√≥ƒá pustƒÖ listƒô.\\n5. Nie generuj ≈ºadnych dodatkowych wyja≈õnie≈Ñ, wstƒôp√≥w ani komentarzy.\\n\\nPrzyk≈Çad wyj≈õcia:\\n{\"discovered_techniques\": [\"EMOTIONAL_CONTENT\", \"EXAGGERATION\"]}'],\n",
              " 'input': ['\"Aborcja to zab√≥jstwo, to jest fakt!\" \"Wyrok Trybuna≈Çu Konstytucyjnego w sprawie zakazu aborcji eugenicznej by≈Ç sukcesem w walce o obronƒô ≈ºycia. Aborcjoni≈õci nie mogli siƒô z tym pogodziƒá, wiƒôc zintensyfikowali morderczƒÖ propagandƒô ‚Äì omijajƒÖc prawo namawiajƒÖ oraz pomagajƒÖ w zabijaniu nienarodzonych dzieci. ≈ölady tej propagandy mogli≈õmy zobaczyƒá w ca≈Çym kraju ‚Äì dewastowano przystanki, billboardy, mury ko≈õcio≈Ç√≥w i zwyk≈Çe domy wypisujƒÖc proaborcyjne has≈Ça oraz piszƒÖc numery telefon√≥w do organizacji pomagajƒÖcych w mordowaniu dzieci. Dlatego trzeba powiedzieƒá jasno: ABORCJA to ZAB√ìJSTWO i uratowaƒá dzieci z rƒÖk aborter√≥w. Tym bardziej, ≈ºe organizacje, kt√≥re siƒô niƒÖ zajmujƒÖ, czujƒÖ siƒô coraz bardziej bezkarne ‚Äì niedawno publicznie og≈Çosi≈Çy, ≈ºe pomagajƒÖ zabijaƒá dzieci nawet w 37. tygodniu ciƒÖ≈ºy!! Przecie≈º te dzieci mog≈Çyby siƒô normalnie urodziƒá i ≈ºyƒá! Dla wszystkich normalnym jest, ≈ºe nie mo≈ºna namawiaƒá do zabicia 3-letniego dziecka czy doros≈Çej osoby. Powszechnie przyjƒôte te≈º jest, ≈ºe nie mo≈ºna promowaƒá zabijania niepe≈Çnosprawnych, oty≈Çych, ≈ªyd√≥w, Francuz√≥w lub Polak√≥w z jakiegokolwiek powodu. Dlaczego wiƒôc nie zakazaƒá propagandy aborcyjnej? To jest mo≈ºliwe i sprawiedliwe. Przyjƒôcie ustawy ‚ÄûAborcja to Zab√≥jstwo‚Äù uratuje ≈ºycie wielu dzieci. Dosyƒá obchodzenia prawa przez aborcjonistki! Obywatelska Inicjatywa Ustawodawcza ‚ÄûABORCJA to ZAB√ìJSTWO‚Äù ma za zadanie zako≈Ñczyƒá ohydne praktyki aborcjonist√≥w i wzmocniƒá ochronƒô ≈ºycia. Projekt ustawy przygotowany przez prawnik√≥w Fundacji ≈ªycie i Rodzina powstrzyma bezkarne pomocnictwo i propagowanie aborcji. Do≈ÇƒÖcz ju≈º dzisiaj. Podpisz siƒô pod projektem. Zbierz podpisy w≈õr√≥d rodziny i przyjaci√≥≈Ç. Zorganizuj zbi√≥rkƒô! aborcjatozabojstwo.pl\"',\n",
              "  '\"Frontex krytykuje Litwƒô za kryzys migracyjny - Dziennik Polityczny\" \"W opinii urzƒôdnik√≥w Frontexu, litewscy funkcjonariusze nie majƒÖ odsy≈Çaƒá z≈Çapanych, nielegalnych migrant√≥w na Bia≈Çoru≈õ, ale powinni zapewniƒá im legalnƒÖ mo≈ºliwo≈õƒá ubiegania siƒô o azyl. Biuro Praw Podstawowych (FRO) agencji Frontex, kt√≥re przeprowadzi≈Ço wizytƒô monitorujƒÖcƒÖ sytuacjƒô na granicy litewsko-bia≈Çoruskiej w zwiƒÖzku z kryzysem migracyjnym, przedstawi≈Ço w tym tygodniu sw√≥j raport. Wed≈Çug serwisu Delfi, skrytykowano w nim stosowanƒÖ prze litewskie s≈Çu≈ºby taktykƒô tzw. push-back. FRO twierdzi, ≈ºe zdoby≈Ço dowody na kolektywne wydalanie nielegalnych migrant√≥w przez litewski s≈Çu≈ºby ochrony granicy. Wed≈Çug biura agencji, praktyka ta jest sprzeczna w prawem miƒôdzynarodowym i EuropejskƒÖ KonwencjƒÖ Praw Cz≈Çowieka. Ponadto, zdaniem urzƒôdnik√≥w, obecny system stosowany przez Litwin√≥w, polegajƒÖcy na zalecaniu migrantom przy≈Çapanym na nielegalnym przekraczaniu granicy, by z≈Ço≈ºyli wnioski azylowy w punkcie granicznym, nie sprawdza siƒô. Biuro przedstawi≈Ço te≈º swoje rekomendacje dla strony litewskiej. Zdaniem urzƒôdnik√≥w, litewscy funkcjonariusze nie powinni odsy≈Çaƒá ‚Äûnieregularnych migrant√≥w‚Äù na Bia≈Çoru≈õ. Zamiast tego zalecajƒÖ, ≈ºeby zabieraƒá ich do punkt√≥w kontroli granicznej, ≈ºeby mogli tam legalnie z≈Ço≈ºyƒá wnioski o azyl. Ponadto, FRO twierdzi, ≈ºe Litwa nie posiada dzia≈ÇajƒÖcego systemu umo≈ºliwiajƒÖcego identyfikacjƒô tzw. os√≥b wra≈ºliwych w≈õr√≥d odsy≈Çanych na Bia≈Çoru≈õ. Chodzi tu np. o powa≈ºnie chorych, rodzic√≥w z ma≈Çymi dzieƒámi, osoby starsze czy kobiety w ciƒÖ≈ºy. We wtorek raport urzƒôdnik√≥w Biura Praw Podstawowych agencji Frontex skomentowa≈Ço MSW Litwy. Potwierdzono otrzymanie rekomendacji, zaznaczajƒÖc, ≈ºe nie majƒÖ one charakteru wiƒÖ≈ºƒÖcego, choƒá nie wykluczono wziƒôcia czƒô≈õci z nich pod uwagƒô. Jednocze≈õnie, ministerstwo podkre≈õli≈Ço w swoim o≈õwiadczeniu, ≈ºe Litwa znajduje siƒô w obliczy kryzysu migracyjnego, sztucznie wywo≈Çanego przez w≈Çadze w Mi≈Ñsku, prowadzƒÖce ‚Äûatak hybrydowy przeciwko Unii Europejskiej poprzez instrumentalne wykorzystywanie migrant√≥w‚Äù. Zaznaczono te≈º, ≈ºe polityka Litwy ma wsparcie ze strony NATO i UE. ‚ÄûDop√≥ki ta sztucznie wywo≈Çana sytuacja z nielegalnƒÖ migracjƒÖ nie ulegnie zmianie, Litwa nie widzi mo≈ºliwo≈õci, by fundamentalnie zmieniƒá obecne ≈õrodki odpowiedzi na granicy z Bia≈ÇorusiƒÖ‚Äù ‚Äì o≈õwiadczy≈Ç wiceszef litewskiego MSW, Arnoldas Abramavicius. Doda≈Ç, ≈ºe Litwini bƒôdƒÖ dalej prowadziƒá politykƒô niewpuszczania migrant√≥w, przynajmniej do czasu, gdy powstanie fizyczne ogrodzenie, a wzd≈Çu≈º ca≈Çej granicy zostanie zainstalowany nowoczesny system monitoringu. Ponadto, w oparciu o do≈õwiadczenie zdobyte przy zarzƒÖdzaniu kryzysem migracyjnym, litewskie ministerstwo spraw wewnƒôtrznych pracuje nad konkretnymi propozycjami na rzecz zmian w Kodeksie Granicznych Schengen i w Europejskim Pakcie o Migracji i Azylu. Wed≈Çug litewskiej stra≈ºy granicznej, od sierpnia br. na granicy z Bia≈ÇorusiƒÖ zawr√≥cono ju≈º oko≈Ço 8 tysiƒôcy migrant√≥w, przy czym czƒô≈õƒá z nich to osoby, kt√≥re pr√≥bowa≈Çy przedrzeƒá siƒô wiƒôcej ni≈º raz. W 2021 roku ponad 4200 os√≥b zdo≈Ça≈Ço nielegalnie przekroczyƒá granicƒô. Litwa i inne kraje zachodnie oskar≈ºajƒÖ w≈Çadze w Mi≈Ñski o organizowanie nap≈Çywu migrant√≥w w bezprecedensowej skali. Stan wyjƒÖtkowy og≈Çoszony w rejonie wzd≈Çu≈º granicy z Bia≈ÇorusiƒÖ, przed≈Çu≈ºono na Litwie do po≈Çowy stycznia 2022 roku.\"'],\n",
              " 'output': ['```json\\n{\"discovered_techniques\": [\"EXAGGERATION\"]}\\n```',\n",
              "  '```json\\n{\"discovered_techniques\": [\"CHERRY_PICKING\"]}\\n```']}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # or \"qwen-2.5\"\n",
        ")\n",
        "\n",
        "# 2. Define the Function for Alpaca Format\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    # Iterate over the 3 columns simultaneously\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Convert your 3 columns into the standard list of messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": instruction},\n",
        "            {\"role\": \"user\", \"content\": input},\n",
        "            {\"role\": \"assistant\", \"content\": output}\n",
        "        ]\n",
        "\n",
        "        # Apply the template to turn the list into a string\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# 3. Apply it\n",
        "train_dataset = train.map(formatting_prompts_func, batched=True)\n",
        "val_dataset = val.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "4Hg7tfMJgn7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad9406d-5e4a-4658-dd3b-b84f048f2b92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[101]['text']"
      ],
      "metadata": {
        "id": "WvH2OHxYoAtb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "e74d13f9-f1ae-4c2c-ce59-57d3d6b7f4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>system\\nJeste≈õ ekspertem ds. weryfikacji informacji i analizy medi√≥w. Twoim zadaniem jest przeanalizowanie podanego artyku≈Çu i zidentyfikowanie w nim technik manipulacji.\\n\\nDozwolone kategorie manipulacji to:\\n- REFERENCE_ERROR\\n- WHATABOUTISM\\n- STRAWMAN\\n- EMOTIONAL_CONTENT\\n- CHERRY_PICKING\\n- FALSE_CAUSE\\n- MISLEADING_CLICKBAIT\\n- ANECDOTE\\n- LEADING_QUESTIONS\\n- EXAGGERATION\\n- QUOTE_MINING\\n\\nInstrukcja:\\n1. Przeanalizuj dok≈Çadnie tre≈õƒá podanego artyku≈Çu.\\n2. Dopasuj fragmenty tekstu do powy≈ºszych definicji.\\n3. Zwr√≥ƒá wynik WY≈ÅƒÑCZNIE w formacie JSON z kluczem \"discovered_techniques\".\\n4. Je≈õli nie wykryto ≈ºadnych technik, zwr√≥ƒá pustƒÖ listƒô.\\n5. Nie generuj ≈ºadnych dodatkowych wyja≈õnie≈Ñ, wstƒôp√≥w ani komentarzy.\\n\\nPrzyk≈Çad wyj≈õcia:\\n{\"discovered_techniques\": [\"EMOTIONAL_CONTENT\", \"EXAGGERATION\"]}<|im_end|>\\n<|im_start|>user\\n\"Aborcja to zab√≥jstwo, to jest fakt!\" \"Wyrok Trybuna≈Çu Konstytucyjnego w sprawie zakazu aborcji eugenicznej by≈Ç sukcesem w walce o obronƒô ≈ºycia. Aborcjoni≈õci nie mogli siƒô z tym pogodziƒá, wiƒôc zintensyfikowali morderczƒÖ propagandƒô ‚Äì omijajƒÖc prawo namawiajƒÖ oraz pomagajƒÖ w zabijaniu nienarodzonych dzieci. ≈ölady tej propagandy mogli≈õmy zobaczyƒá w ca≈Çym kraju ‚Äì dewastowano przystanki, billboardy, mury ko≈õcio≈Ç√≥w i zwyk≈Çe domy wypisujƒÖc proaborcyjne has≈Ça oraz piszƒÖc numery telefon√≥w do organizacji pomagajƒÖcych w mordowaniu dzieci. Dlatego trzeba powiedzieƒá jasno: ABORCJA to ZAB√ìJSTWO i uratowaƒá dzieci z rƒÖk aborter√≥w. Tym bardziej, ≈ºe organizacje, kt√≥re siƒô niƒÖ zajmujƒÖ, czujƒÖ siƒô coraz bardziej bezkarne ‚Äì niedawno publicznie og≈Çosi≈Çy, ≈ºe pomagajƒÖ zabijaƒá dzieci nawet w 37. tygodniu ciƒÖ≈ºy!! Przecie≈º te dzieci mog≈Çyby siƒô normalnie urodziƒá i ≈ºyƒá! Dla wszystkich normalnym jest, ≈ºe nie mo≈ºna namawiaƒá do zabicia 3-letniego dziecka czy doros≈Çej osoby. Powszechnie przyjƒôte te≈º jest, ≈ºe nie mo≈ºna promowaƒá zabijania niepe≈Çnosprawnych, oty≈Çych, ≈ªyd√≥w, Francuz√≥w lub Polak√≥w z jakiegokolwiek powodu. Dlaczego wiƒôc nie zakazaƒá propagandy aborcyjnej? To jest mo≈ºliwe i sprawiedliwe. Przyjƒôcie ustawy ‚ÄûAborcja to Zab√≥jstwo‚Äù uratuje ≈ºycie wielu dzieci. Dosyƒá obchodzenia prawa przez aborcjonistki! Obywatelska Inicjatywa Ustawodawcza ‚ÄûABORCJA to ZAB√ìJSTWO‚Äù ma za zadanie zako≈Ñczyƒá ohydne praktyki aborcjonist√≥w i wzmocniƒá ochronƒô ≈ºycia. Projekt ustawy przygotowany przez prawnik√≥w Fundacji ≈ªycie i Rodzina powstrzyma bezkarne pomocnictwo i propagowanie aborcji. Do≈ÇƒÖcz ju≈º dzisiaj. Podpisz siƒô pod projektem. Zbierz podpisy w≈õr√≥d rodziny i przyjaci√≥≈Ç. Zorganizuj zbi√≥rkƒô! aborcjatozabojstwo.pl\"<|im_end|>\\n<|im_start|>assistant\\n```json\\n{\"discovered_techniques\": [\"EXAGGERATION\"]}\\n```<|im_end|>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Init trainer"
      ],
      "metadata": {
        "id": "RV3KAALRVTl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this once to get your total dataset size\n",
        "with open(train_data_path, \"r\") as f:\n",
        "    total_samples = sum(1 for _ in f)\n",
        "\n",
        "print(f\"Total samples: {total_samples}\")\n",
        "\n",
        "steps_in_epoch = total_samples // (BATCH_SIZE * GRAD_ACCUM)\n",
        "\n",
        "print(\"Steps in epoch:\", steps_in_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IemnCswODxw",
        "outputId": "14ed3285-ff87-408b-d21d-8b6bc1bf8186"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 10749\n",
            "Steps in epoch: 1343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "\n",
        "        per_device_train_batch_size = BATCH_SIZE,\n",
        "        gradient_accumulation_steps = GRAD_ACCUM, # Use GA to mimic batch size!\n",
        "        dataloader_num_workers = 0,\n",
        "\n",
        "        max_steps = steps_in_epoch*2, #2 epochs\n",
        "\n",
        "        output_dir = \"/content/drive/MyDrive/unsloth_bielik_4_5B_sft\",\n",
        "        save_steps = 100,\n",
        "        save_total_limit = 3,\n",
        "\n",
        "        warmup_steps = 50,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "wmJRVVvXjN4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759c4505-b292-4ff5-9454-14d62ba87528"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:trl.trainer.sft_trainer:You are using a per_device_train_batch_size of 1 with padding-free training. Using a batch size of 1 anihilate the benefits of padding-free training. Please consider increasing the batch size to at least 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|im_start|>user\\n\",\n",
        "    response_part = \"<|im_start|>assistant\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "XsoGsOY-jTHo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(trainer.train_dataset[101][\"input_ids\"])"
      ],
      "metadata": {
        "id": "NPkUBWYZjV5I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "14f3cdca-e502-4dc2-80ff-99376588a522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s><|im_start|> system\\nJeste≈õ ekspertem ds. weryfikacji informacji i analizy medi√≥w. Twoim zadaniem jest przeanalizowanie podanego artyku≈Çu i zidentyfikowanie w nim technik manipulacji.\\n\\nDozwolone kategorie manipulacji to:\\n- REFERENCE_ERROR\\n- WHATABOUTISM\\n- STRAWMAN\\n- EMOTIONAL_CONTENT\\n- CHERRY_PICKING\\n- FALSE_CAUSE\\n- MISLEADING_CLICKBAIT\\n- ANECDOTE\\n- LEADING_QUESTIONS\\n- EXAGGERATION\\n- QUOTE_MINING\\n\\nInstrukcja:\\n1. Przeanalizuj dok≈Çadnie tre≈õƒá podanego artyku≈Çu.\\n2. Dopasuj fragmenty tekstu do powy≈ºszych definicji.\\n3. Zwr√≥ƒá wynik WY≈ÅƒÑCZNIE w formacie JSON z kluczem \"discovered_techniques\".\\n4. Je≈õli nie wykryto ≈ºadnych technik, zwr√≥ƒá pustƒÖ listƒô.\\n5. Nie generuj ≈ºadnych dodatkowych wyja≈õnie≈Ñ, wstƒôp√≥w ani komentarzy.\\n\\nPrzyk≈Çad wyj≈õcia:\\n{\"discovered_techniques\": [\"EMOTIONAL_CONTENT\", \"EXAGGERATION\"]}<|im_end|> \\n<|im_start|> user\\n\"Aborcja to zab√≥jstwo, to jest fakt!\" \"Wyrok Trybuna≈Çu Konstytucyjnego w sprawie zakazu aborcji eugenicznej by≈Ç sukcesem w walce o obronƒô ≈ºycia. Aborcjoni≈õci nie mogli siƒô z tym pogodziƒá, wiƒôc zintensyfikowali morderczƒÖ propagandƒô ‚Äì omijajƒÖc prawo namawiajƒÖ oraz pomagajƒÖ w zabijaniu nienarodzonych dzieci. ≈ölady tej propagandy mogli≈õmy zobaczyƒá w ca≈Çym kraju ‚Äì dewastowano przystanki, billboardy, mury ko≈õcio≈Ç√≥w i zwyk≈Çe domy wypisujƒÖc proaborcyjne has≈Ça oraz piszƒÖc numery telefon√≥w do organizacji pomagajƒÖcych w mordowaniu dzieci. Dlatego trzeba powiedzieƒá jasno: ABORCJA to ZAB√ìJSTWO i uratowaƒá dzieci z rƒÖk aborter√≥w. Tym bardziej, ≈ºe organizacje, kt√≥re siƒô niƒÖ zajmujƒÖ, czujƒÖ siƒô coraz bardziej bezkarne ‚Äì niedawno publicznie og≈Çosi≈Çy, ≈ºe pomagajƒÖ zabijaƒá dzieci nawet w 37. tygodniu ciƒÖ≈ºy!! Przecie≈º te dzieci mog≈Çyby siƒô normalnie urodziƒá i ≈ºyƒá! Dla wszystkich normalnym jest, ≈ºe nie mo≈ºna namawiaƒá do zabicia 3-letniego dziecka czy doros≈Çej osoby. Powszechnie przyjƒôte te≈º jest, ≈ºe nie mo≈ºna promowaƒá zabijania niepe≈Çnosprawnych, oty≈Çych, ≈ªyd√≥w, Francuz√≥w lub Polak√≥w z jakiegokolwiek powodu. Dlaczego wiƒôc nie zakazaƒá propagandy aborcyjnej? To jest mo≈ºliwe i sprawiedliwe. Przyjƒôcie ustawy ‚ÄûAborcja to Zab√≥jstwo‚Äù uratuje ≈ºycie wielu dzieci. Dosyƒá obchodzenia prawa przez aborcjonistki! Obywatelska Inicjatywa Ustawodawcza ‚ÄûABORCJA to ZAB√ìJSTWO‚Äù ma za zadanie zako≈Ñczyƒá ohydne praktyki aborcjonist√≥w i wzmocniƒá ochronƒô ≈ºycia. Projekt ustawy przygotowany przez prawnik√≥w Fundacji ≈ªycie i Rodzina powstrzyma bezkarne pomocnictwo i propagowanie aborcji. Do≈ÇƒÖcz ju≈º dzisiaj. Podpisz siƒô pod projektem. Zbierz podpisy w≈õr√≥d rodziny i przyjaci√≥≈Ç. Zorganizuj zbi√≥rkƒô! aborcjatozabojstwo.pl\"<|im_end|> \\n<|im_start|> assistant\\n```json\\n{\"discovered_techniques\": [\"EXAGGERATION\"]}\\n```<|im_end|> \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[101][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ],
      "metadata": {
        "id": "mBlS7w00oxIW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8aa67b37-dac0-455d-815d-c160a6cbe3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ```json\\n{\"discovered_techniques\": []}\\n```<|im_end|> \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "wVR-EgL6ozPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b116db3-b7cf-4e3d-e6f3-250ff586fc47"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "4.498 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run sft"
      ],
      "metadata": {
        "id": "X67HwS3lVXJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "cRysTD8ho1l3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69d20876-84b2-4b44-c57c-bb293cd4bf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 21,488 | Num Epochs = 9,223,372,036,854,775,807 | Total steps = 2,686\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 99,778,560 of 4,857,038,848 (2.05% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='69' max='2686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  69/2686 30:47 < 20:02:51, 0.04 it/s, Epoch 0.03/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.889800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.843000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.817700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.863000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.231400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.112500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.044900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.032800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.136100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.030900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.068600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.121600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.109700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.066700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.082600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.145300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.080500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.097500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.113600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.084500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.017000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.065000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.111800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.061900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.064800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.043400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.049000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.102900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.080100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.003700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.055200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.071500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.054400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.030600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.030900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.018500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.059100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.062700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.072900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.038900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.049700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.054500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.028300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.066500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.036700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.060600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.088100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.080800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.074300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.060700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.079100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.065600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.048500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.020700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.047700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.078200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.048000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.059900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.105300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.030100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.033000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.043700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.065600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.082900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "A_16CDVvo4PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = val[0]\n",
        "\n",
        "messages = [\n",
        "    {\"role\" : \"system\", \"content\" : message[\"instruction\"]},\n",
        "    {\"role\" : \"user\", \"content\" : message[\"input\"]}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 1000, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")\n",
        "\n",
        "print(\"--------\")\n",
        "print(\"Correct label: \", message[\"output\"])"
      ],
      "metadata": {
        "id": "x0XZzAcypLZP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}