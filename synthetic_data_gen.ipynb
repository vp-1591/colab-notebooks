{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "MMChwx16mRWn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CONFIGURATION ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TzkoJpu1iFRA",
        "outputId": "02949d6a-04bb-439d-df2b-68f1dbbc513d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_FILE = \"/content/drive/MyDrive/mipd_train_16k.jsonl\"\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/mipd_train_cot.jsonl\"\n",
        "\n",
        "TEACHER_MODEL = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
        "MAX_SEQ_LENGTH = 16384 # 16 k to process full article length\n",
        "MAX_NEW_TOKENS = 512\n",
        "MAX_RETRIES = 3\n",
        "SAMPLE_LIMIT = None # Set to None for full processing\n",
        "\n",
        "# --- LOAD MODEL ---\n",
        "print(f\"Loading Teacher Model: {TEACHER_MODEL}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = TEACHER_MODEL,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# --- RESUME LOGIC ---\n",
        "processed_inputs = set()\n",
        "\n",
        "if os.path.exists(OUTPUT_FILE):\n",
        "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "                processed_inputs.add(entry['input'])\n",
        "            except: pass\n",
        "\n",
        "print(f\"RESUME: Found {len(processed_inputs)} already processed samples.\")\n",
        "\n",
        "# --- HELPERS ---\n",
        "def clean_json_string(text):\n",
        "    return text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "# --- MAIN LOOP ---\n",
        "print(\"Starting Synthetic Dataset Generation...\")\n",
        "\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    all_lines = f.readlines()\n",
        "\n",
        "if SAMPLE_LIMIT:\n",
        "    all_lines = all_lines[:SAMPLE_LIMIT]\n",
        "\n",
        "\n",
        "with open(OUTPUT_FILE, 'a', encoding='utf-8') as f_out:\n",
        "\n",
        "    for i, line in tqdm(enumerate(all_lines), total=len(all_lines), desc=\"Generating\"):\n",
        "        data = json.loads(line)\n",
        "        user_input = data['input']\n",
        "\n",
        "        if user_input in processed_inputs:\n",
        "            continue\n",
        "\n",
        "        raw_json_str = clean_json_string(data['output'])\n",
        "\n",
        "        # --- HARDENED PROMPT ---\n",
        "        system_prompt = (\n",
        "            \"JesteÅ› surowym sÄ™dziÄ… i ekspertem od dezinformacji. \"\n",
        "            \"Twoim zadaniem jest uzasadnienie werdyktu dotyczÄ…cego manipulacji w tekÅ›cie.\"\n",
        "        )\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"TEKST:\\n\\\"{user_input}\\\"\\n\\n\"\n",
        "            f\"WERDYKT (Techniki): {raw_json_str}\\n\\n\"\n",
        "            \"Napisz uzasadnienie dla tego werdyktu. \"\n",
        "            \"Zasady:\\n\"\n",
        "            \"1. Pisz w trybie orzekajÄ…cym (np. 'Autor stosuje...', 'Tekst zawiera...'). \"\n",
        "            \"2. NIE uÅ¼ywaj sÅ‚Ã³w niepewnoÅ›ci ('wydaje siÄ™', 'byÄ‡ moÅ¼e', 'prawdopodobnie').\\n\"\n",
        "            \"3. Traktuj podane techniki jako FAKT. WyjaÅ›nij GDZIE i DLACZEGO wystÄ™pujÄ….\\n\"\n",
        "            \"4. JeÅ›li lista technik jest pusta, napisz wprost: 'Tekst ma charakter informacyjny i nie zawiera cech manipulacji.'\\n\"\n",
        "            \"5. Wygeneruj TYLKO treÅ›Ä‡ uzasadnienia (bez cudzysÅ‚owÃ³w i formatowania).\\n\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "\n",
        "        input_ids = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        success = False\n",
        "        reasoning_text = \"\"\n",
        "\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(\n",
        "                        input_ids=input_ids,\n",
        "                        max_new_tokens=MAX_NEW_TOKENS,\n",
        "                        use_cache=True,\n",
        "                        temperature=0.7,\n",
        "                        pad_token_id=tokenizer.eos_token_id\n",
        "                    )\n",
        "\n",
        "                generated_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "                # Basic cleanup\n",
        "                generated_text = generated_text.replace('\"', \"'\") # Replace double quotes to safe single quotes\n",
        "\n",
        "                if len(generated_text) > 10:\n",
        "                    reasoning_text = generated_text\n",
        "                    success = True\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if not success:\n",
        "            continue\n",
        "\n",
        "        # --- CONSTRUCT FINAL JSON OBJECT ---\n",
        "        # We perform the merge here in Python to guarantee valid JSON structure for the UI\n",
        "        try:\n",
        "            techniques_list = json.loads(raw_json_str).get(\"discovered_techniques\", [])\n",
        "        except:\n",
        "            techniques_list = []\n",
        "\n",
        "        final_structure = {\n",
        "            \"reasoning\": reasoning_text,\n",
        "            \"discovered_techniques\": techniques_list\n",
        "        }\n",
        "\n",
        "        # Save dataset entry\n",
        "        new_entry = data.copy()\n",
        "        # This 'output' is now a perfect stringified JSON ready for SFT\n",
        "        new_entry['output'] = json.dumps(final_structure, ensure_ascii=False)\n",
        "        new_entry['original_output'] = raw_json_str\n",
        "\n",
        "        f_out.write(json.dumps(new_entry, ensure_ascii=False) + \"\\n\")\n",
        "        f_out.flush()\n",
        "\n",
        "        if SAMPLE_LIMIT and i < SAMPLE_LIMIT:\n",
        "            print(f\"\\n--- Sample {i+1} ---\")\n",
        "            print(json.dumps(final_structure, indent=2, ensure_ascii=False))\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBeecjx1mnWo",
        "outputId": "c517d4a2-9e35-4fa9-bdeb-2825b3b9d164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Loading Teacher Model: unsloth/Qwen2.5-7B-Instruct-bnb-4bit...\n",
            "==((====))==  Unsloth 2026.1.3: Fast Qwen2 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "RESUME: Found 0 already processed samples.\n",
            "Starting Synthetic Dataset Generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating:   0%|          | 0/10733 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Generating:   5%|â–         | 533/10733 [1:03:10<20:31:17,  7.24s/it]Unsloth: Input IDs of shape torch.Size([1, 16882]) with length 16882 > the model's max sequence length of 16384.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Generating:  10%|â–‰         | 1054/10733 [2:09:17<24:32:26,  9.13s/it]Unsloth: Input IDs of shape torch.Size([1, 17476]) with length 17476 > the model's max sequence length of 16384.\n",
            "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
            "Generating:  15%|â–ˆâ–Œ        | 1612/10733 [3:11:48<10:55:12,  4.31s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_FILE = \"/content/drive/MyDrive/mipd_train_cot.jsonl\"\n",
        "\n",
        "record_count = 0\n",
        "try:\n",
        "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            record_count += 1\n",
        "    print(f\"Total records in '{OUTPUT_FILE}': {record_count}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Output file '{OUTPUT_FILE}' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "ZahRBhW4ZjIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "skipped_count = len(all_lines) - record_count\n",
        "print(f\"Done. Skipped {skipped_count} problematic samples.\")"
      ],
      "metadata": {
        "id": "5P6owOL7Y3Jj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}