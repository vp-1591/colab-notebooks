{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "MMChwx16mRWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CONFIGURATION ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TzkoJpu1iFRA",
        "outputId": "dee95fec-7e5f-4a1d-a41a-0e06c49de8ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_FILE = \"/content/drive/MyDrive/mipd_train_16k.jsonl\"\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/mipd_train_cot.jsonl\"\n",
        "DENIED_SAMPLES_FILE = \"/content/drive/MyDrive/denied_samples.txt\" # New logging file\n",
        "CHECKPOINT_FILE = \"/content/drive/MyDrive/checkpoint.txt\"  # NEW: Stores the current index\n",
        "\n",
        "TEACHER_MODEL = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
        "MAX_SEQ_LENGTH = 16384 # 16 k to process full article length\n",
        "MAX_NEW_TOKENS = 512\n",
        "MAX_RETRIES = 3\n",
        "SAMPLE_LIMIT = None # Set to None for full processing\n",
        "\n",
        "# --- LOAD MODEL ---\n",
        "print(f\"Loading Teacher Model: {TEACHER_MODEL}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = TEACHER_MODEL,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# --- RESUME LOGIC ---\n",
        "start_index = 0\n",
        "\n",
        "if os.path.exists(CHECKPOINT_FILE):\n",
        "    with open(CHECKPOINT_FILE, 'r') as f:\n",
        "        try:\n",
        "            content = f.read().strip()\n",
        "            if content:\n",
        "                start_index = int(content) + 1 # Start from the NEXT item\n",
        "        except ValueError:\n",
        "            start_index = 0\n",
        "\n",
        "print(f\"RESUME: Starting from index {start_index}\")\n",
        "\n",
        "# --- HELPERS ---\n",
        "def clean_json_string(text):\n",
        "    return text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "def validate_reasoning(reasoning, techniques):\n",
        "    for t in techniques:\n",
        "        # Check if technique name (case-insensitive) is in the reasoning\n",
        "        if t.lower() not in reasoning.lower():\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# --- MAIN LOOP ---\n",
        "print(\"Starting Synthetic Dataset Generation...\")\n",
        "\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    all_lines = f.readlines()\n",
        "\n",
        "if SAMPLE_LIMIT:\n",
        "    all_lines = all_lines[:SAMPLE_LIMIT]\n",
        "\n",
        "lines_to_process = all_lines[start_index:]\n",
        "print(f\"Processing {len(lines_to_process)} remaining samples...\")\n",
        "\n",
        "with open(OUTPUT_FILE, 'a', encoding='utf-8') as f_out:\n",
        "\n",
        "    for i, line in tqdm(enumerate(lines_to_process, start=start_index), total=len(lines_to_process)):\n",
        "        data = json.loads(line)\n",
        "        user_input = data['input']\n",
        "        raw_json_str = clean_json_string(data['output'])\n",
        "\n",
        "        # Extract techniques list upfront\n",
        "        try:\n",
        "            techniques_list = json.loads(raw_json_str).get(\"discovered_techniques\", [])\n",
        "        except:\n",
        "            techniques_list = []\n",
        "\n",
        "        success = False\n",
        "        reasoning_text = \"\"\n",
        "\n",
        "        if not techniques_list:\n",
        "            # Hardcode the response if no techniques are found\n",
        "            reasoning_text = \"Tekst ma charakter informacyjny i nie zawiera cech manipulacji.\"\n",
        "            success = True\n",
        "        else:\n",
        "            # --- HARDENED PROMPT ---\n",
        "            system_prompt = (\n",
        "                \"JesteÅ› surowym sÄ™dziÄ… i ekspertem od dezinformacji. \"\n",
        "                \"Twoim zadaniem jest uzasadnienie werdyktu dotyczÄ…cego manipulacji w tekÅ›cie.\"\n",
        "            )\n",
        "\n",
        "            user_prompt = (\n",
        "                f\"TEKST:\\n\\\"{user_input}\\\"\\n\\n\"\n",
        "                f\"WERDYKT (Techniki): {raw_json_str}\\n\\n\"\n",
        "                \"Napisz uzasadnienie dla tego werdyktu. \"\n",
        "                \"Zasady:\\n\"\n",
        "                \"1. Pisz w trybie orzekajÄ…cym (np. 'Autor stosuje...', 'Tekst zawiera...'). \"\n",
        "                \"2. NIE uÅ¼ywaj sÅ‚Ã³w niepewnoÅ›ci ('wydaje siÄ™', 'byÄ‡ moÅ¼e', 'prawdopodobnie').\\n\"\n",
        "                \"3. Traktuj podane techniki jako FAKT. WyjaÅ›nij GDZIE i DLACZEGO wystÄ™pujÄ….\\n\"\n",
        "                \"4. Wygeneruj TYLKO treÅ›Ä‡ uzasadnienia (bez cudzysÅ‚owÃ³w i formatowania).\\n\"\n",
        "                \"5. KaÅ¼da technika z listy MUSI byÄ‡ wymieniona z nazwy dokÅ‚adnie tak jak w werdykcie.\\n\"\n",
        "                \"6. Nie wolno omawiaÄ‡ technik, ktÃ³rych nie ma na liÅ›cie.\\n\"\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "\n",
        "            input_ids = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=True,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            for attempt in range(MAX_RETRIES):\n",
        "                try:\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model.generate(\n",
        "                            input_ids=input_ids,\n",
        "                            max_new_tokens=MAX_NEW_TOKENS,\n",
        "                            attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
        "                            use_cache=True,\n",
        "                            temperature=0.2,\n",
        "                            top_p=0.9,\n",
        "                            pad_token_id=tokenizer.eos_token_id\n",
        "                        )\n",
        "\n",
        "                    generated_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "                    if not validate_reasoning(generated_text, techniques_list):\n",
        "                        with open(DENIED_SAMPLES_FILE, 'a', encoding='utf-8') as f_denied:\n",
        "                            f_denied.write(f\"--- Denied Sample {i+1} ---\\n\")\n",
        "                            f_denied.write(f\"User Prompt:\\n{user_prompt}\\n\")\n",
        "                            f_denied.write(f\"Generated Text:\\n{generated_text}\\n\")\n",
        "                            f_denied.write(f\"Techniques List: {techniques_list}\\n\\n\")\n",
        "                        continue  # skip sample if reasoning doesn't mention techniques\n",
        "\n",
        "                    if len(generated_text) > 10:\n",
        "                        reasoning_text = generated_text\n",
        "                        success = True\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating text for sample {i+1}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if not success:\n",
        "            continue\n",
        "\n",
        "        # --- CONSTRUCT FINAL JSON OBJECT ---\n",
        "        # We perform the merge here in Python to guarantee valid JSON structure for the UI\n",
        "        final_structure = {\n",
        "            \"reasoning\": reasoning_text,\n",
        "            \"discovered_techniques\": techniques_list\n",
        "        }\n",
        "\n",
        "        # Save dataset entry\n",
        "        new_entry = data.copy()\n",
        "        # This 'output' is now a perfect stringified JSON ready for SFT\n",
        "        new_entry['output'] = json.dumps(final_structure, ensure_ascii=False)\n",
        "        new_entry['original_output'] = raw_json_str\n",
        "\n",
        "        f_out.write(json.dumps(new_entry, ensure_ascii=False) + \"\\n\")\n",
        "        f_out.flush()\n",
        "\n",
        "        if SAMPLE_LIMIT and i < SAMPLE_LIMIT:\n",
        "            print(f\"\\n--- Sample {i+1} ---\")\n",
        "            print(json.dumps(final_structure, indent=2, ensure_ascii=False))\n",
        "\n",
        "        with open(CHECKPOINT_FILE, 'w') as f_ckpt:\n",
        "            f_ckpt.write(str(i))\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBeecjx1mnWo",
        "outputId": "7ef21edc-1fe8-4e86-907d-3dcfe4fa597f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Loading Teacher Model: unsloth/Qwen2.5-7B-Instruct-bnb-4bit...\n",
            "==((====))==  Unsloth 2026.1.3: Fast Qwen2 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "RESUME: Found 2176 already processed samples.\n",
            "Starting Synthetic Dataset Generation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating:   0%|          | 0/10733 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_FILE = \"/content/drive/MyDrive/mipd_train_cot.jsonl\"\n",
        "\n",
        "record_count = 0\n",
        "try:\n",
        "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            record_count += 1\n",
        "    print(f\"Total records in '{OUTPUT_FILE}': {record_count}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Output file '{OUTPUT_FILE}' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "ZahRBhW4ZjIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355efd8c-2279-4462-bb09-f14d88c8d720"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records in '/content/drive/MyDrive/mipd_train_cot.jsonl': 2180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "INPUT_FILE = \"/content/drive/MyDrive/mipd_train_16k.jsonl\"\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    all_lines = f.readlines()\n",
        "print(record_count)\n",
        "print(len(all_lines))\n",
        "skipped_count = len(all_lines) - record_count\n",
        "print(f\"Done. Skipped {skipped_count} problematic samples.\")"
      ],
      "metadata": {
        "id": "5P6owOL7Y3Jj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbea5d45-9c4f-4548-ba63-7657a2c73183"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2180\n",
            "10733\n",
            "Done. Skipped 8553 problematic samples.\n"
          ]
        }
      ]
    }
  ]
}