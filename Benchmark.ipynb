{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5438e59cadc34a6485d02ed0f412aa7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_673a9f91d8e349ddb54d8847b98e04b5",
              "IPY_MODEL_fd92efc8f52c4ec7a5b2b5ca408405c6",
              "IPY_MODEL_5d2c132de4654143a22f4079c33587d2"
            ],
            "layout": "IPY_MODEL_095729d152194ce6a679bb47b3e3a06d"
          }
        },
        "673a9f91d8e349ddb54d8847b98e04b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48a264415aa14805affc0d09c0a9abc9",
            "placeholder": "​",
            "style": "IPY_MODEL_75e2a52533114b92a251c3d0d5143d11",
            "value": "Map: 100%"
          }
        },
        "fd92efc8f52c4ec7a5b2b5ca408405c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6c71bf4568d44f5aa499f505d10aa39",
            "max": 1521,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_185932d0185643cdb978d0612a16c404",
            "value": 1521
          }
        },
        "5d2c132de4654143a22f4079c33587d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98bef515db4f4fd9a6f1483d44b4f5ec",
            "placeholder": "​",
            "style": "IPY_MODEL_360808f2a88d4502b5a7c61dcfc201b9",
            "value": " 1521/1521 [00:00&lt;00:00, 5172.09 examples/s]"
          }
        },
        "095729d152194ce6a679bb47b3e3a06d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a264415aa14805affc0d09c0a9abc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e2a52533114b92a251c3d0d5143d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6c71bf4568d44f5aa499f505d10aa39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185932d0185643cdb978d0612a16c404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98bef515db4f4fd9a6f1483d44b4f5ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "360808f2a88d4502b5a7c61dcfc201b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import libs"
      ],
      "metadata": {
        "id": "yp6KMqYL_LBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "RQRC97HcyHnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95be7d5b",
        "outputId": "a0f5feca-09a0-4e9e-d7b1-9419320da6b8"
      },
      "source": [
        "!pip install --quiet vllm scikit-learn tqdm\n",
        "print(\"Required libraries unsloth, vllm, scikit-learn, and tqdm installed successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required libraries unsloth, vllm, scikit-learn, and tqdm installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to drive for test dataset"
      ],
      "metadata": {
        "id": "dj7X1r7Y_Qo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROgYyP62M3SW",
        "outputId": "33fa0881-da82-47db-c634-d6ffdce4b3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and tokenizer"
      ],
      "metadata": {
        "id": "M47uCXWI_aen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_path = \"drive/MyDrive/mipd_test.jsonl\"\n",
        "MAX_NEW_TOKENS = 512\n",
        "max_seq_length = 16384\n",
        "base_model_dir = \"drive/MyDrive/bielik-4.5b-base\"\n",
        "TEST_ROWS = None # None for whole dataset\n",
        "lora_path = \"/content/drive/MyDrive/unsloth_bielik_4_5B_sft_cot/checkpoint-2475\" #None for base model"
      ],
      "metadata": {
        "id": "0_f3y0eLsiz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = lora_path if lora_path else base_model_dir,\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")"
      ],
      "metadata": {
        "id": "XferaHy_ipFJ",
        "outputId": "d1c225cd-335c-4f6a-d610-2450821ed1d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.15.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and prepare dataset"
      ],
      "metadata": {
        "id": "N5OMhg3y_iaW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "5438e59cadc34a6485d02ed0f412aa7a",
            "673a9f91d8e349ddb54d8847b98e04b5",
            "fd92efc8f52c4ec7a5b2b5ca408405c6",
            "5d2c132de4654143a22f4079c33587d2",
            "095729d152194ce6a679bb47b3e3a06d",
            "48a264415aa14805affc0d09c0a9abc9",
            "75e2a52533114b92a251c3d0d5143d11",
            "a6c71bf4568d44f5aa499f505d10aa39",
            "185932d0185643cdb978d0612a16c404",
            "98bef515db4f4fd9a6f1483d44b4f5ec",
            "360808f2a88d4502b5a7c61dcfc201b9"
          ]
        },
        "id": "014141ff",
        "outputId": "59ec91e5-13a7-4fe5-c2dc-a2e1b85af1ca"
      },
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the test dataset\n",
        "dataset_test = load_dataset(\"json\", data_files={'test': test_data_path})\n",
        "\n",
        "# 3. Define a function named format_prompt\n",
        "def format_prompt(example):\n",
        "    # Combine instruction for system message and input for the user message\n",
        "    system_instruction = '''\n",
        "Jesteś ekspertem w dziedzinie analizy mediów i lingwistyki, specjalizującym się w wykrywaniu propagandy, manipulacji poznawczej i błędów logicznych w tekstach w języku polskim.\n",
        "\n",
        "**Twoje zadanie:**\n",
        "Przeanalizuj dostarczony tekst wejściowy w języku polskim, aby zidentyfikować konkretne techniki manipulacji. Musisz oprzeć swoją analizę wyłącznie na dostarczonym tekście, szukając wzorców, które mają na celu wpłynięcie na opinię czytelnika za pomocą środków irracjonalnych lub zwodniczych.\n",
        "\n",
        "**Dozwolone kategorie manipulacji:**\n",
        "Jesteś ściśle ograniczony do klasyfikowania technik w następujących kategoriach. Nie używaj żadnych innych tagów.\n",
        "\n",
        "1.  **REFERENCE_ERROR**: Cytaty, które nie popierają tezy, są zmyślone lub pochodzą z niewiarygodnych źródeł.\n",
        "2.  **WHATABOUTISM**: Dyskredytowanie stanowiska oponenta poprzez zarzucanie mu hipokryzji, bez bezpośredniego odparcia jego argumentów.\n",
        "3.  **STRAWMAN**: Przeinaczenie argumentu oponenta (stworzenie \"chochoła\"), aby łatwiej go było zaatakować.\n",
        "4.  **EMOTIONAL_CONTENT**: Używanie języka nasyconego emocjami (strach, gniew, litość, radość) w celu ominięcia racjonalnego, krytycznego myślenia.\n",
        "5.  **CHERRY_PICKING**: Zatajanie dowodów lub ignorowanie danych, które zaprzeczają argumentowi, przy jednoczesnym przedstawianiu tylko danych potwierdzających.\n",
        "6.  **FALSE_CAUSE**: Błędne zidentyfikowanie przyczyny zjawiska (np. mylenie korelacji z przyczynowością).\n",
        "7.  **MISLEADING_CLICKBAIT**: Nagłówki lub wstępy, które sensacyjnie wyolbrzymiają lub fałszywie przedstawiają faktyczną treść tekstu.\n",
        "8.  **ANECDOTE**: Wykorzystywanie odosobnionych historii osobistych lub pojedynczych przykładów jako ważnego dowodu na ogólny trend lub fakt naukowy.\n",
        "9.  **LEADING_QUESTIONS**: Pytania sformułowane w sposób sugerujący konkretną odpowiedź lub zawierające nieudowodnione założenie.\n",
        "10. **EXAGGERATION**: Hiperboliczne stwierdzenia, które wyolbrzymiają fakty, aby wywołać reakcję.\n",
        "11. **QUOTE_MINING**: Wyrywanie cytatów z kontekstu w celu zniekształcenia intencji pierwotnego autora.\n",
        "\n",
        "**Format wyjściowy:**\n",
        "Musisz odpowiedzieć pojedynczym, poprawnym obiektem JSON zawierającym dwa klucze:\n",
        "1.  `\"reasoning\"`: Spójny akapit w **języku polskim** wyjaśniający, które techniki znaleziono i dlaczego. Musisz przytoczyć konkretną logikę lub fragmenty tekstu, aby uzasadnić swoją klasyfikację.\n",
        "2.  `\"discovered_techniques\"`: Lista ciągów znaków (stringów) zawierająca dokładnie te tagi, które zdefiniowano powyżej. Jeśli nie znaleziono żadnych technik, zwróć pustą listę.\n",
        "\n",
        "**Przykładowa struktura:**\n",
        "{\n",
        "    \"reasoning\": \"Tekst stosuje [Nazwa Techniki], ponieważ autor sugeruje, że...\",\n",
        "    \"discovered_techniques\": [\"NAZWA_TECHNIKI\"]\n",
        "}\n",
        "    '''\n",
        "    user_message = example['input']\n",
        "\n",
        "    # Construct the ChatML formatted prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_instruction},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ]\n",
        "    example['prompt'] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # 5. Parse the output field into a Python list of strings (ground truth tags)\n",
        "    try:\n",
        "        # CLEANING STEP: Remove Markdown formatting\n",
        "        clean_json = example['output'].replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        example['tags'] = json.loads(clean_json)['discovered_techniques']\n",
        "    except json.JSONDecodeError:\n",
        "        # Handle cases where output might not be perfectly valid JSON (e.g., during training data prep)\n",
        "        example['tags'] = [] # Assign empty list if parsing fails\n",
        "        print(f\"Warning: Could not parse output: {example['output']}\")\n",
        "\n",
        "    # 6. Return the modified example\n",
        "    return example\n",
        "\n",
        "\n",
        "if(TEST_ROWS):\n",
        "  small_test_dataset = dataset_test['test'].select(range(TEST_ROWS))\n",
        "else:\n",
        "  small_test_dataset = dataset_test['test']\n",
        "# 7. Apply the format_prompt function to the loaded test dataset\n",
        "original_columns = small_test_dataset.column_names\n",
        "dataset_test_formatted = small_test_dataset.map(format_prompt, remove_columns=original_columns)\n",
        "\n",
        "print(\"Formatted prompts and ground truth tags generated for the test dataset.\")\n",
        "print(dataset_test_formatted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1521 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5438e59cadc34a6485d02ed0f412aa7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted prompts and ground truth tags generated for the test dataset.\n",
            "Dataset({\n",
            "    features: ['prompt', 'tags'],\n",
            "    num_rows: 1521\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define evaluation function\n",
        "Metric 1: Parsing Success Rate (Did it output valid JSON?).\n",
        "\n",
        "Metric 2: Format Correction Rate (How many invalid jsons were recovered?)\n",
        "\n",
        "Metric 3: Classification Performance (If parsable, how accurate?).\n"
      ],
      "metadata": {
        "id": "5NAGs3A6AJ9C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c58a713"
      },
      "source": [
        "import json\n",
        "import re\n",
        "# from sklearn.metrics import f1_score # No longer used directly for the document-level F1 as defined by the user\n",
        "\n",
        "def evaluate_response(response_text: str, ground_truth_tags: list):\n",
        "    \"\"\"\n",
        "    Evaluates response with support for Dict format {\"discovered_techniques\": []}\n",
        "    and Markdown stripping.\n",
        "    \"\"\"\n",
        "    parsed_tags = []\n",
        "    parsing_status = 'Failed'\n",
        "\n",
        "    # 0. Pre-processing: Strip Markdown (Crucial for Strict Success)\n",
        "    clean_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "    # Attempt 1: Strict JSON parsing\n",
        "    try:\n",
        "        parsed_output = json.loads(clean_text)\n",
        "\n",
        "        # CASE A: Output is the expected Dictionary\n",
        "        if isinstance(parsed_output, dict):\n",
        "            # Extract the specific key we trained on\n",
        "            parsed_tags = parsed_output.get(\"discovered_techniques\", [])\n",
        "            # Check if the inner content is actually a list\n",
        "            if not isinstance(parsed_tags, list):\n",
        "                 # Try to force it if it's a string representation\n",
        "                 parsed_tags = []\n",
        "            parsing_status = 'Strict Success'\n",
        "\n",
        "        # CASE B: Model outputted a raw List (unlikely but possible)\n",
        "        elif isinstance(parsed_output, list):\n",
        "            parsed_tags = parsed_output\n",
        "            parsing_status = 'Strict Success'\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Parsed output is not a Dict or List.\")\n",
        "\n",
        "    except (json.JSONDecodeError, ValueError):\n",
        "        # Attempt 2: Regex-based correction\n",
        "        # We look for the list explicitly\n",
        "        match = re.search(r'\\[(.*?)\\]', clean_text, re.DOTALL)\n",
        "        if match:\n",
        "            extracted_content = f\"[{match.group(1)}]\"\n",
        "            try:\n",
        "                parsed_output_recovered = json.loads(extracted_content)\n",
        "                if isinstance(parsed_output_recovered, list):\n",
        "                    parsed_tags = parsed_output_recovered\n",
        "                    parsing_status = 'Recovered'\n",
        "            except (json.JSONDecodeError, ValueError):\n",
        "                pass\n",
        "\n",
        "    # Clean tags and convert to sets for easier set operations\n",
        "    parsed_tags_set = set(str(tag) for tag in parsed_tags if tag is not None)\n",
        "    ground_truth_tags_set = set(str(tag) for tag in ground_truth_tags if tag is not None)\n",
        "\n",
        "    # --- Document-level F1 calculation (as per user definition) ---\n",
        "    # TP = |pred ∩ gold|\n",
        "    tp_doc = len(parsed_tags_set.intersection(ground_truth_tags_set))\n",
        "    # FP = |pred − gold|\n",
        "    fp_doc = len(parsed_tags_set.difference(ground_truth_tags_set))\n",
        "    # FN = |gold − pred|\n",
        "    fn_doc = len(ground_truth_tags_set.difference(parsed_tags_set))\n",
        "\n",
        "    # F1_doc = 0 if TP=FP=FN=0, else 2*TP / (2*TP + FP + FN)\n",
        "    if tp_doc == 0 and fp_doc == 0 and fn_doc == 0:\n",
        "        f1_doc = 0.0 # Per user instruction for when both sets are empty\n",
        "    else:\n",
        "        f1_doc = (2 * tp_doc) / (2 * tp_doc + fp_doc + fn_doc)\n",
        "\n",
        "    # Exact-match accuracy\n",
        "    exact_match = (parsed_tags_set == ground_truth_tags_set)\n",
        "\n",
        "    return {\n",
        "        'parsing_status': parsing_status,\n",
        "        'parsed_tags': list(parsed_tags_set), # Store as list for consistency\n",
        "        'f1_doc': f1_doc,\n",
        "        'exact_match': exact_match,\n",
        "        'has_gold_labels': bool(ground_truth_tags_set), # To identify documents with non-empty gold labels\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer and evaluate all answers"
      ],
      "metadata": {
        "id": "p6jZqmG4Ar-o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "331f55f4",
        "outputId": "3a65f2c3-9028-4cf6-86e7-6749ead9dcaf"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "print(f\"Loading Model with max_seq_length = {max_seq_length}...\")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# --- 2. SIMPLE INFERENCE LOOP (No Chunks!) ---\n",
        "print(\"Starting Long-Context Inference...\")\n",
        "evaluation_results = []\n",
        "\n",
        "for example in tqdm(dataset_test_formatted, desc=\"Processing\"):\n",
        "    prompt = example['prompt']\n",
        "    ground_truth_tags = example['tags']\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    # Since we have a massive context, we just feed the whole thing in.\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens = 256,\n",
        "            use_cache = True,\n",
        "            do_sample = False,\n",
        "            temperature = 0.0,\n",
        "             # Unsloth handles padding automatically usually, but being explicit is safe\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    # Slice off the input prompt\n",
        "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
        "    raw_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # --- 3. STANDARD EVALUATION ---\n",
        "    # Clean JSON\n",
        "    if '}' in raw_output:\n",
        "        raw_output = raw_output[:raw_output.find('}') + 1]\n",
        "\n",
        "    # Evaluate\n",
        "    result = evaluate_response(raw_output, ground_truth_tags)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'original_prompt_len': inputs.input_ids.shape[1], # Log length to verify it worked\n",
        "        'ground_truth': ground_truth_tags,\n",
        "        'predicted': result['parsed_tags'],\n",
        "        'f1_doc': result['f1_doc'], # Store new document-level F1\n",
        "        'exact_match': result['exact_match'], # Store exact match result\n",
        "        'parsing_status': result['parsing_status'], # Added parsing_status here\n",
        "        'raw_output': raw_output,\n",
        "        'has_gold_labels': result['has_gold_labels'] # Indicate if document had gold labels\n",
        "    })\n",
        "\n",
        "    # CRITICAL: Clear cache after massive context to avoid OOM on next iteration\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "print(f\"Done. Processed {len(evaluation_results)} documents.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model with max_seq_length = 16384...\n",
            "Starting Long-Context Inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing:  16%|█▌        | 240/1521 [39:11<2:25:30,  6.82s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import multilabel_confusion_matrix # Import for confusion matrix\n",
        "\n",
        "# --- 4. REPORTING ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"INFERENCE REPORT: {len(evaluation_results)} documents\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if evaluation_results:\n",
        "    total_docs = len(evaluation_results)\n",
        "    strict_success_count = 0\n",
        "    recovered_count = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    all_ground_truth_for_cm = []\n",
        "    all_predicted_for_cm = []\n",
        "    all_unique_tags_set = set()\n",
        "\n",
        "    # New metrics for document-level F1 and exact match\n",
        "    total_f1_doc = 0.0\n",
        "    total_f1_doc_non_empty_gold = 0.0\n",
        "    non_empty_gold_docs_count = 0\n",
        "    exact_matches_count = 0\n",
        "\n",
        "    # Add comments explaining why document-level F1 is needed\n",
        "    print(\"\\n### Importance of Document-Level F1 Score ###\")\n",
        "    print(\"Document-level F1 is crucial for evaluating multi-label classification models, especially when dealing with imbalanced datasets or documents with varying numbers of labels (including documents with no ground truth labels). Unlike traditional micro or macro F1 scores which aggregate metrics across all labels, document-level F1 assesses the quality of predictions for each individual document. This approach prevents models from artificially inflating performance by correctly predicting 'no labels' for many documents, or by performing well on frequently occurring labels while failing on rare ones. It provides a more faithful representation of the model's ability to assign the correct set of labels to each document.\")\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    for r in evaluation_results:\n",
        "        if r['parsing_status'] == 'Strict Success':\n",
        "            strict_success_count += 1\n",
        "        elif r['parsing_status'] == 'Recovered':\n",
        "            recovered_count += 1\n",
        "        else:\n",
        "            failed_count += 1\n",
        "\n",
        "        all_ground_truth_for_cm.append(r['ground_truth'])\n",
        "        all_predicted_for_cm.append(r['predicted'])\n",
        "        all_unique_tags_set.update(r['ground_truth'])\n",
        "        all_unique_tags_set.update(r['predicted'])\n",
        "\n",
        "        total_f1_doc += r['f1_doc']\n",
        "        if r['has_gold_labels']:\n",
        "            total_f1_doc_non_empty_gold += r['f1_doc']\n",
        "            non_empty_gold_docs_count += 1\n",
        "\n",
        "        if r['exact_match']:\n",
        "            exact_matches_count += 1\n",
        "\n",
        "    parsing_success_rate = strict_success_count / total_docs\n",
        "    format_correction_rate = recovered_count / total_docs\n",
        "    total_parsing_success = (strict_success_count + recovered_count) / total_docs\n",
        "\n",
        "    print(f\"\\nParsing Success Rate (Strict JSON): {parsing_success_rate:.4f} ({strict_success_count}/{total_docs})\")\n",
        "    print(f\"Format Correction Rate (Recovered JSON): {format_correction_rate:.4f} ({recovered_count}/{total_docs})\")\n",
        "    print(f\"Total Parsing Success (Strict + Recovered): {total_parsing_success:.4f} ({strict_success_count + recovered_count}/{total_docs})\")\n",
        "\n",
        "    # New document-level metrics\n",
        "    mean_f1_doc_all_docs = total_f1_doc / total_docs\n",
        "    print(f\"\\nMean Document-Level F1 (all docs): {mean_f1_doc_all_docs:.4f}\")\n",
        "\n",
        "    if non_empty_gold_docs_count > 0:\n",
        "        mean_f1_doc_non_empty = total_f1_doc_non_empty_gold / non_empty_gold_docs_count\n",
        "        print(f\"Mean Document-Level F1 (excluding empty gold-label docs): {mean_f1_doc_non_empty:.4f}\")\n",
        "    else:\n",
        "        print(\"Mean Document-Level F1 (excluding empty gold-label docs): N/A (No documents with gold labels found)\")\n",
        "\n",
        "    exact_match_accuracy = exact_matches_count / total_docs\n",
        "    print(f\"Exact-Match Accuracy: {exact_match_accuracy:.4f} ({exact_matches_count}/{total_docs})\")\n",
        "\n",
        "    # Prepare data for Confusion Matrix (existing logic, unchanged)\n",
        "    all_unique_tags = sorted(list(all_unique_tags_set))\n",
        "    if not all_unique_tags:\n",
        "        print(\"\\nNo unique tags found in ground truth or predictions for confusion matrix.\")\n",
        "    else:\n",
        "        y_true_bin = [[1 if tag in gt_list else 0 for tag in all_unique_tags] for gt_list in all_ground_truth_for_cm]\n",
        "        y_pred_bin = [[1 if tag in pred_list else 0 for tag in all_unique_tags] for pred_list in all_predicted_for_cm]\n",
        "        cm = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
        "\n",
        "        print(\"\\n--- Confusion Matrix (per tag) ---\")\n",
        "        for i, tag in enumerate(all_unique_tags):\n",
        "            tn, fp, fn, tp = cm[i].ravel()\n",
        "            print(f\"Tag: {tag}\")\n",
        "            print(f\"  True Negatives (TN): {tn}\")\n",
        "            print(f\"  False Positives (FP): {fp}\")\n",
        "            print(f\"  False Negatives (FN): {fn}\")\n",
        "            print(f\"  True Positives (TP): {tp}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "\n",
        "    # 2. Print Sample Table\n",
        "    print(\"\\n--- Sample Results (First 10) ---\")\n",
        "    print(f\"{'F1_Doc':<10} | {'Exact Match':<12} | {'Ground Truth':<30} | {'Model Output (Reasoning & Tags)':<80}\")\n",
        "    print(\"-\" * 135)\n",
        "\n",
        "    for res in evaluation_results[:10]:\n",
        "        gt_str = str(res['ground_truth'])\n",
        "        gt_display = (gt_str[:100] + '..') if len(gt_str) > 30 else gt_str\n",
        "\n",
        "        output_display = \"\"\n",
        "        try:\n",
        "            parsed_output = json.loads(res['raw_output'])\n",
        "            reasoning = parsed_output.get('reasoning', 'No reasoning provided')\n",
        "            discovered_techniques = parsed_output.get('discovered_techniques', [])\n",
        "            output_display = f\"Reasoning: {reasoning[:100]}... | Tags: {str(discovered_techniques)[:100]}...\"\n",
        "        except json.JSONDecodeError:\n",
        "            output_display = (res['raw_output'][:80] + '..') if len(res['raw_output']) > 80 else res['raw_output']\n",
        "\n",
        "        print(f\"{res['f1_doc']:.4f}     | {str(res['exact_match']):<12} | {gt_display:<30} | {output_display:<80}\")\n",
        "\n",
        "    print(\"-\" * 135)\n",
        "else:\n",
        "    print(\"No results generated.\")"
      ],
      "metadata": {
        "id": "FBxs_MwGBQz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f48fd899"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Initialize aggregated confusion matrix components\n",
        "total_tn = 0\n",
        "total_fp = 0\n",
        "total_fn = 0\n",
        "total_tp = 0\n",
        "\n",
        "# Aggregate confusion matrix components from individual tag matrices\n",
        "for i, tag in enumerate(all_unique_tags):\n",
        "    tn, fp, fn, tp = cm[i].ravel()\n",
        "    total_tn += tn\n",
        "    total_fp += fp\n",
        "    total_fn += fn\n",
        "    total_tp += tp\n",
        "\n",
        "# Create the combined confusion matrix\n",
        "combined_cm = np.array([\n",
        "    [total_tn, total_fp],\n",
        "    [total_fn, total_tp]\n",
        "])\n",
        "\n",
        "# Plot the combined confusion matrix\n",
        "fig = plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(combined_cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted No', 'Predicted Yes'],\n",
        "            yticklabels=['Actual No', 'Actual Yes'])\n",
        "plt.title('Combined Confusion Matrix for All Tags')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}