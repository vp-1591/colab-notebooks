{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import libs"
      ],
      "metadata": {
        "id": "yp6KMqYL_LBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "RQRC97HcyHnc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95be7d5b",
        "outputId": "27abce99-72ab-4fe8-86a1-d3afa140d04b"
      },
      "source": [
        "!pip install --quiet scikit-learn tqdm\n",
        "print(\"Required libraries unsloth, scikit-learn, and tqdm installed successfully.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required libraries unsloth, scikit-learn, and tqdm installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to drive for test dataset and model"
      ],
      "metadata": {
        "id": "dj7X1r7Y_Qo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROgYyP62M3SW",
        "outputId": "6c4fb632-0e81-4e9a-9ebc-bb1374bcfa4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and tokenizer"
      ],
      "metadata": {
        "id": "M47uCXWI_aen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_path = \"drive/MyDrive/mipd_test.jsonl\"\n",
        "MAX_NEW_TOKENS = 512\n",
        "max_seq_length = 16384\n",
        "base_model_dir = \"drive/MyDrive/bielik-4.5b-base\"\n",
        "TEST_ROWS = None # None for whole dataset\n",
        "lora_path = None #None for base model"
      ],
      "metadata": {
        "id": "0_f3y0eLsiz5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = lora_path if lora_path else base_model_dir,\n",
        "    max_seq_length = max_seq_length, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")"
      ],
      "metadata": {
        "id": "XferaHy_ipFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c0339c5-7247-424e-9db5-22356d0864c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and prepare dataset"
      ],
      "metadata": {
        "id": "N5OMhg3y_iaW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "014141ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b79f5a-a8f2-4541-9070-5f098e09a19f"
      },
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the test dataset\n",
        "dataset_test = load_dataset(\"json\", data_files={'test': test_data_path})\n",
        "\n",
        "# 3. Define a function named format_prompt\n",
        "def format_prompt(example):\n",
        "    # Combine instruction for system message and input for the user message\n",
        "    system_instruction = '''\n",
        "JesteÅ› ekspertem w dziedzinie analizy mediÃ³w i lingwistyki, specjalizujÄ…cym siÄ™ w wykrywaniu propagandy, manipulacji poznawczej i bÅ‚Ä™dÃ³w logicznych w tekstach w jÄ™zyku polskim.\n",
        "\n",
        "**Twoje zadanie:**\n",
        "Przeanalizuj dostarczony tekst wejÅ›ciowy w jÄ™zyku polskim, aby zidentyfikowaÄ‡ konkretne techniki manipulacji. Musisz oprzeÄ‡ swojÄ… analizÄ™ wyÅ‚Ä…cznie na dostarczonym tekÅ›cie, szukajÄ…c wzorcÃ³w, ktÃ³re majÄ… na celu wpÅ‚yniÄ™cie na opiniÄ™ czytelnika za pomocÄ… Å›rodkÃ³w irracjonalnych lub zwodniczych.\n",
        "\n",
        "**Dozwolone kategorie manipulacji:**\n",
        "JesteÅ› Å›ciÅ›le ograniczony do klasyfikowania technik w nastÄ™pujÄ…cych kategoriach. Nie uÅ¼ywaj Å¼adnych innych tagÃ³w.\n",
        "\n",
        "1.  **REFERENCE_ERROR**: Cytaty, ktÃ³re nie popierajÄ… tezy, sÄ… zmyÅ›lone lub pochodzÄ… z niewiarygodnych ÅºrÃ³deÅ‚.\n",
        "2.  **WHATABOUTISM**: Dyskredytowanie stanowiska oponenta poprzez zarzucanie mu hipokryzji, bez bezpoÅ›redniego odparcia jego argumentÃ³w.\n",
        "3.  **STRAWMAN**: Przeinaczenie argumentu oponenta (stworzenie \"chochoÅ‚a\"), aby Å‚atwiej go byÅ‚o zaatakowaÄ‡.\n",
        "4.  **EMOTIONAL_CONTENT**: UÅ¼ywanie jÄ™zyka nasyconego emocjami (strach, gniew, litoÅ›Ä‡, radoÅ›Ä‡) w celu ominiÄ™cia racjonalnego, krytycznego myÅ›lenia.\n",
        "5.  **CHERRY_PICKING**: Zatajanie dowodÃ³w lub ignorowanie danych, ktÃ³re zaprzeczajÄ… argumentowi, przy jednoczesnym przedstawianiu tylko danych potwierdzajÄ…cych.\n",
        "6.  **FALSE_CAUSE**: BÅ‚Ä™dne zidentyfikowanie przyczyny zjawiska (np. mylenie korelacji z przyczynowoÅ›ciÄ…).\n",
        "7.  **MISLEADING_CLICKBAIT**: NagÅ‚Ã³wki lub wstÄ™py, ktÃ³re sensacyjnie wyolbrzymiajÄ… lub faÅ‚szywie przedstawiajÄ… faktycznÄ… treÅ›Ä‡ tekstu.\n",
        "8.  **ANECDOTE**: Wykorzystywanie odosobnionych historii osobistych lub pojedynczych przykÅ‚adÃ³w jako waÅ¼nego dowodu na ogÃ³lny trend lub fakt naukowy.\n",
        "9.  **LEADING_QUESTIONS**: Pytania sformuÅ‚owane w sposÃ³b sugerujÄ…cy konkretnÄ… odpowiedÅº lub zawierajÄ…ce nieudowodnione zaÅ‚oÅ¼enie.\n",
        "10. **EXAGGERATION**: Hiperboliczne stwierdzenia, ktÃ³re wyolbrzymiajÄ… fakty, aby wywoÅ‚aÄ‡ reakcjÄ™.\n",
        "11. **QUOTE_MINING**: Wyrywanie cytatÃ³w z kontekstu w celu znieksztaÅ‚cenia intencji pierwotnego autora.\n",
        "\n",
        "**Format wyjÅ›ciowy:**\n",
        "Musisz odpowiedzieÄ‡ pojedynczym, poprawnym obiektem JSON zawierajÄ…cym dwa klucze:\n",
        "1.  `\"reasoning\"`: SpÃ³jny akapit w **jÄ™zyku polskim** wyjaÅ›niajÄ…cy, ktÃ³re techniki znaleziono i dlaczego. Musisz przytoczyÄ‡ konkretnÄ… logikÄ™ lub fragmenty tekstu, aby uzasadniÄ‡ swojÄ… klasyfikacjÄ™.\n",
        "2.  `\"discovered_techniques\"`: Lista ciÄ…gÃ³w znakÃ³w (stringÃ³w) zawierajÄ…ca dokÅ‚adnie te tagi, ktÃ³re zdefiniowano powyÅ¼ej. JeÅ›li nie znaleziono Å¼adnych technik, zwrÃ³Ä‡ pustÄ… listÄ™.\n",
        "\n",
        "**PrzykÅ‚adowa struktura:**\n",
        "{\n",
        "    \"reasoning\": \"Tekst stosuje [Nazwa Techniki], poniewaÅ¼ autor sugeruje, Å¼e...\",\n",
        "    \"discovered_techniques\": [\"NAZWA_TECHNIKI\"]\n",
        "}\n",
        "    '''\n",
        "    user_message = example['input']\n",
        "\n",
        "    # Construct the ChatML formatted prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_instruction},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ]\n",
        "    example['prompt'] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # 5. Parse the output field into a Python list of strings (ground truth tags)\n",
        "    try:\n",
        "        # CLEANING STEP: Remove Markdown formatting\n",
        "        clean_json = example['output'].replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        example['tags'] = json.loads(clean_json)['discovered_techniques']\n",
        "    except json.JSONDecodeError:\n",
        "        # Handle cases where output might not be perfectly valid JSON (e.g., during training data prep)\n",
        "        example['tags'] = [] # Assign empty list if parsing fails\n",
        "        print(f\"Warning: Could not parse output: {example['output']}\")\n",
        "\n",
        "    # 6. Return the modified example\n",
        "    return example\n",
        "\n",
        "\n",
        "if(TEST_ROWS):\n",
        "  small_test_dataset = dataset_test['test'].select(range(TEST_ROWS))\n",
        "else:\n",
        "  small_test_dataset = dataset_test['test']\n",
        "# 7. Apply the format_prompt function to the loaded test dataset\n",
        "original_columns = small_test_dataset.column_names\n",
        "dataset_test_formatted = small_test_dataset.map(format_prompt, remove_columns=original_columns)\n",
        "\n",
        "print(\"Formatted prompts and ground truth tags generated for the test dataset.\")\n",
        "print(dataset_test_formatted)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted prompts and ground truth tags generated for the test dataset.\n",
            "Dataset({\n",
            "    features: ['prompt', 'tags'],\n",
            "    num_rows: 1521\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define evaluation function\n",
        "Metric 1: Parsing Success Rate (Did it output valid JSON?).\n",
        "\n",
        "Metric 2: Format Correction Rate (How many invalid jsons were recovered?)\n",
        "\n",
        "Metric 3: Classification Performance (If parsable, how accurate?).\n"
      ],
      "metadata": {
        "id": "5NAGs3A6AJ9C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c58a713"
      },
      "source": [
        "import json\n",
        "import re\n",
        "# from sklearn.metrics import f1_score # No longer used directly for the document-level F1 as defined by the user\n",
        "\n",
        "def evaluate_response(response_text: str, ground_truth_tags: list):\n",
        "    \"\"\"\n",
        "    Evaluates response with support for Dict format {\"discovered_techniques\": []}\n",
        "    and Markdown stripping.\n",
        "    \"\"\"\n",
        "    parsed_tags = []\n",
        "    parsing_status = 'Failed'\n",
        "\n",
        "    # 0. Pre-processing: Strip Markdown (Crucial for Strict Success)\n",
        "    clean_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "    # Attempt 1: Strict JSON parsing\n",
        "    try:\n",
        "        parsed_output = json.loads(clean_text)\n",
        "\n",
        "        # CASE A: Output is the expected Dictionary\n",
        "        if isinstance(parsed_output, dict):\n",
        "            # Extract the specific key we trained on\n",
        "            parsed_tags = parsed_output.get(\"discovered_techniques\", [])\n",
        "            # Check if the inner content is actually a list\n",
        "            if not isinstance(parsed_tags, list):\n",
        "                 # Try to force it if it's a string representation\n",
        "                 parsed_tags = []\n",
        "            parsing_status = 'Strict Success'\n",
        "\n",
        "        # CASE B: Model outputted a raw List (unlikely but possible)\n",
        "        elif isinstance(parsed_output, list):\n",
        "            parsed_tags = parsed_output\n",
        "            parsing_status = 'Strict Success'\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Parsed output is not a Dict or List.\")\n",
        "\n",
        "    except (json.JSONDecodeError, ValueError):\n",
        "        # Attempt 2: Regex-based correction\n",
        "        # We look for the list explicitly\n",
        "        match = re.search(r'\\[(.*?)\\]', clean_text, re.DOTALL)\n",
        "        if match:\n",
        "            extracted_content = f\"[{match.group(1)}]\"\n",
        "            try:\n",
        "                parsed_output_recovered = json.loads(extracted_content)\n",
        "                if isinstance(parsed_output_recovered, list):\n",
        "                    parsed_tags = parsed_output_recovered\n",
        "                    parsing_status = 'Recovered'\n",
        "            except (json.JSONDecodeError, ValueError):\n",
        "                pass\n",
        "\n",
        "    # Clean tags and convert to sets for easier set operations\n",
        "    parsed_tags_set = set(str(tag) for tag in parsed_tags if tag is not None)\n",
        "    ground_truth_tags_set = set(str(tag) for tag in ground_truth_tags if tag is not None)\n",
        "\n",
        "    # --- Document-level F1 calculation (as per user definition) ---\n",
        "    # TP = |pred âˆ© gold|\n",
        "    tp_doc = len(parsed_tags_set.intersection(ground_truth_tags_set))\n",
        "    # FP = |pred âˆ’ gold|\n",
        "    fp_doc = len(parsed_tags_set.difference(ground_truth_tags_set))\n",
        "    # FN = |gold âˆ’ pred|\n",
        "    fn_doc = len(ground_truth_tags_set.difference(parsed_tags_set))\n",
        "\n",
        "    # F1_doc = 0 if TP=FP=FN=0, else 2*TP / (2*TP + FP + FN)\n",
        "    if tp_doc == 0 and fp_doc == 0 and fn_doc == 0:\n",
        "        f1_doc = 1.0 # Per user instruction for when both sets are empty\n",
        "    else:\n",
        "        f1_doc = (2 * tp_doc) / (2 * tp_doc + fp_doc + fn_doc)\n",
        "\n",
        "    # Exact-match accuracy\n",
        "    exact_match = (parsed_tags_set == ground_truth_tags_set)\n",
        "\n",
        "    return {\n",
        "        'parsing_status': parsing_status,\n",
        "        'parsed_tags': list(parsed_tags_set), # Store as list for consistency\n",
        "        'f1_doc': f1_doc,\n",
        "        'exact_match': exact_match,\n",
        "        'has_gold_labels': bool(ground_truth_tags_set), # To identify documents with non-empty gold labels\n",
        "    }"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load progress"
      ],
      "metadata": {
        "id": "x75ySAGYEtX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "checkpoint_file_path = \"drive/MyDrive/evaluation_checkpoint.json\"\n",
        "\n",
        "evaluation_results = []\n",
        "start_index = 0\n",
        "\n",
        "if os.path.exists(checkpoint_file_path):\n",
        "    with open(checkpoint_file_path, 'r') as f:\n",
        "        evaluation_results = json.load(f)\n",
        "    start_index = len(evaluation_results)\n",
        "    print(f\"Loaded {start_index} previous results from checkpoint: {checkpoint_file_path}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting from scratch.\")"
      ],
      "metadata": {
        "id": "NaiN0ZU7ExVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593d4f5a-61a5-4e71-d52a-150c429c780e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 600 previous results from checkpoint: drive/MyDrive/evaluation_checkpoint.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer all answers"
      ],
      "metadata": {
        "id": "p6jZqmG4Ar-o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "331f55f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7da609e-7a89-4fcb-aad4-1479b43d7903"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "print(f\"Loading Model with max_seq_length = {max_seq_length}...\")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# --- 2. SIMPLE INFERENCE LOOP (No Chunks!) ---\n",
        "print(\"Starting Long-Context Inference...\")\n",
        "\n",
        "# `evaluation_results` and `start_index` are loaded from checkpoint in cell NaiN0ZU7ExVm\n",
        "# if you want to restart from scratch, run cell NaiN0ZU7ExVm and then empty the `evaluation_results` list and set `start_index` to 0\n",
        "\n",
        "CHECKPOINT_FREQUENCY = 20 # Save every 20 iterations\n",
        "\n",
        "for i, example in enumerate(tqdm(dataset_test_formatted,\n",
        "                                 desc=\"Processing\",\n",
        "                                 initial=start_index,\n",
        "                                 total=len(dataset_test_formatted)),\n",
        "                            start=start_index):\n",
        "    if i < start_index:\n",
        "        continue # Skip already processed examples\n",
        "\n",
        "    prompt = example['prompt']\n",
        "    ground_truth_tags = example['tags']\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    # Since we have a massive context, we just feed the whole thing in.\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens = 256,\n",
        "            use_cache = True,\n",
        "            do_sample = False,\n",
        "            temperature = 0.0,\n",
        "             # Unsloth handles padding automatically usually, but being explicit is safe\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    # Slice off the input prompt\n",
        "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
        "    raw_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # --- 3. STANDARD EVALUATION ---\n",
        "    # Clean JSON\n",
        "    if '}' in raw_output:\n",
        "        raw_output = raw_output[:raw_output.find('}') + 1]\n",
        "\n",
        "    # Evaluate\n",
        "    result = evaluate_response(raw_output, ground_truth_tags)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'original_prompt_len': inputs.input_ids.shape[1], # Log length to verify it worked\n",
        "        'ground_truth': ground_truth_tags,\n",
        "        'predicted': result['parsed_tags'],\n",
        "        'f1_doc': result['f1_doc'], # Store new document-level F1\n",
        "        'exact_match': result['exact_match'], # Store exact match result\n",
        "        'parsing_status': result['parsing_status'], # Added parsing_status here\n",
        "        'raw_output': raw_output,\n",
        "        'has_gold_labels': result['has_gold_labels'] # Indicate if document had gold labels\n",
        "    })\n",
        "\n",
        "    # CRITICAL: Clear cache after massive context to avoid OOM on next iteration\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Save checkpoint periodically\n",
        "    if (i + 1) % CHECKPOINT_FREQUENCY == 0:\n",
        "        with open(checkpoint_file_path, 'w') as f:\n",
        "            json.dump(evaluation_results, f)\n",
        "        print(f\"Checkpoint saved at iteration {i + 1}. Total results: {len(evaluation_results)}\")\n",
        "\n",
        "# Save final results after loop completes\n",
        "with open(checkpoint_file_path, 'w') as f:\n",
        "    json.dump(evaluation_results, f)\n",
        "\n",
        "print(f\"Done. Processed {len(evaluation_results)} documents.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model with max_seq_length = 16384...\n",
            "Starting Long-Context Inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 601/1521 [00:23<5:54:37, 23.13s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate all answers"
      ],
      "metadata": {
        "id": "Nfkt3ewwlJVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import multilabel_confusion_matrix # Import for confusion matrix\n",
        "\n",
        "# --- 4. REPORTING ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"INFERENCE REPORT: {len(evaluation_results)} documents\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if evaluation_results:\n",
        "    total_docs = len(evaluation_results)\n",
        "    strict_success_count = 0\n",
        "    recovered_count = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    all_ground_truth_for_cm = []\n",
        "    all_predicted_for_cm = []\n",
        "    all_unique_tags_set = set()\n",
        "\n",
        "    # New metrics for document-level F1 and exact match\n",
        "    total_f1_doc = 0.0\n",
        "    total_f1_doc_non_empty_gold = 0.0\n",
        "    non_empty_gold_docs_count = 0\n",
        "    exact_matches_count = 0\n",
        "\n",
        "    # Add comments explaining why document-level F1 is needed\n",
        "    print(\"\\n### Importance of Document-Level F1 Score ###\")\n",
        "    print(\"Document-level F1 is crucial for evaluating multi-label classification models, especially when dealing with imbalanced datasets or documents with varying numbers of labels (including documents with no ground truth labels). Unlike traditional micro or macro F1 scores which aggregate metrics across all labels, document-level F1 assesses the quality of predictions for each individual document. This approach prevents models from artificially inflating performance by correctly predicting 'no labels' for many documents, or by performing well on frequently occurring labels while failing on rare ones. It provides a more faithful representation of the model's ability to assign the correct set of labels to each document.\")\n",
        "    print(\"----------------------------------------------\\n\")\n",
        "\n",
        "    for r in evaluation_results:\n",
        "        if r['parsing_status'] == 'Strict Success':\n",
        "            strict_success_count += 1\n",
        "        elif r['parsing_status'] == 'Recovered':\n",
        "            recovered_count += 1\n",
        "        else:\n",
        "            failed_count += 1\n",
        "\n",
        "        all_ground_truth_for_cm.append(r['ground_truth'])\n",
        "        all_predicted_for_cm.append(r['predicted'])\n",
        "        all_unique_tags_set.update(r['ground_truth'])\n",
        "        all_unique_tags_set.update(r['predicted'])\n",
        "\n",
        "        total_f1_doc += r['f1_doc']\n",
        "        if r['has_gold_labels']:\n",
        "            total_f1_doc_non_empty_gold += r['f1_doc']\n",
        "            non_empty_gold_docs_count += 1\n",
        "\n",
        "        if r['exact_match']:\n",
        "            exact_matches_count += 1\n",
        "\n",
        "    parsing_success_rate = strict_success_count / total_docs\n",
        "    format_correction_rate = recovered_count / total_docs\n",
        "    total_parsing_success = (strict_success_count + recovered_count) / total_docs\n",
        "\n",
        "    print(f\"\\nParsing Success Rate (Strict JSON): {parsing_success_rate:.4f} ({strict_success_count}/{total_docs})\")\n",
        "    print(f\"Format Correction Rate (Recovered JSON): {format_correction_rate:.4f} ({recovered_count}/{total_docs})\")\n",
        "    print(f\"Total Parsing Success (Strict + Recovered): {total_parsing_success:.4f} ({strict_success_count + recovered_count}/{total_docs})\")\n",
        "\n",
        "    # New document-level metrics\n",
        "    mean_f1_doc_all_docs = total_f1_doc / total_docs\n",
        "    print(f\"\\nMean Document-Level F1 (all docs): {mean_f1_doc_all_docs:.4f}\")\n",
        "\n",
        "    if non_empty_gold_docs_count > 0:\n",
        "        mean_f1_doc_non_empty = total_f1_doc_non_empty_gold / non_empty_gold_docs_count\n",
        "        print(f\"Mean Document-Level F1 (excluding empty gold-label docs): {mean_f1_doc_non_empty:.4f}\")\n",
        "    else:\n",
        "        print(\"Mean Document-Level F1 (excluding empty gold-label docs): N/A (No documents with gold labels found)\")\n",
        "\n",
        "    exact_match_accuracy = exact_matches_count / total_docs\n",
        "    print(f\"Exact-Match Accuracy: {exact_match_accuracy:.4f} ({exact_matches_count}/{total_docs})\")\n",
        "\n",
        "    # Prepare data for Confusion Matrix (existing logic, unchanged)\n",
        "    all_unique_tags = sorted(list(all_unique_tags_set))\n",
        "    if not all_unique_tags:\n",
        "        print(\"\\nNo unique tags found in ground truth or predictions for confusion matrix.\")\n",
        "    else:\n",
        "        y_true_bin = [[1 if tag in gt_list else 0 for tag in all_unique_tags] for gt_list in all_ground_truth_for_cm]\n",
        "        y_pred_bin = [[1 if tag in pred_list else 0 for tag in all_unique_tags] for pred_list in all_predicted_for_cm]\n",
        "        cm = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
        "\n",
        "        print(\"\\n--- Confusion Matrix (per tag) ---\")\n",
        "        for i, tag in enumerate(all_unique_tags):\n",
        "            tn, fp, fn, tp = cm[i].ravel()\n",
        "            print(f\"Tag: {tag}\")\n",
        "            print(f\"  True Negatives (TN): {tn}\")\n",
        "            print(f\"  False Positives (FP): {fp}\")\n",
        "            print(f\"  False Negatives (FN): {fn}\")\n",
        "            print(f\"  True Positives (TP): {tp}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "\n",
        "    # 2. Print Sample Table\n",
        "    print(\"\\n--- Sample Results (First 10) ---\")\n",
        "    print(f\"{'F1_Doc':<10} | {'Exact Match':<12} | {'Ground Truth':<30} | {'Model Output (Reasoning & Tags)':<80}\")\n",
        "    print(\"-\" * 135)\n",
        "\n",
        "    for res in evaluation_results[:10]:\n",
        "        gt_str = str(res['ground_truth'])\n",
        "        gt_display = (gt_str[:100] + '..') if len(gt_str) > 30 else gt_str\n",
        "\n",
        "        output_display = \"\"\n",
        "        try:\n",
        "            parsed_output = json.loads(res['raw_output'])\n",
        "            reasoning = parsed_output.get('reasoning', 'No reasoning provided')\n",
        "            discovered_techniques = parsed_output.get('discovered_techniques', [])\n",
        "            output_display = f\"Reasoning: {reasoning[:100]}... | Tags: {str(discovered_techniques)[:100]}...\"\n",
        "        except json.JSONDecodeError:\n",
        "            output_display = (res['raw_output'][:80] + '..') if len(res['raw_output']) > 80 else res['raw_output']\n",
        "\n",
        "        print(f\"{res['f1_doc']:.4f}     | {str(res['exact_match']):<12} | {gt_display:<30} | {output_display:<80}\")\n",
        "\n",
        "    print(\"-\" * 135)\n",
        "else:\n",
        "    print(\"No results generated.\")"
      ],
      "metadata": {
        "id": "FBxs_MwGBQz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f48fd899"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Initialize aggregated confusion matrix components\n",
        "total_tn = 0\n",
        "total_fp = 0\n",
        "total_fn = 0\n",
        "total_tp = 0\n",
        "\n",
        "# Aggregate confusion matrix components from individual tag matrices\n",
        "for i, tag in enumerate(all_unique_tags):\n",
        "    tn, fp, fn, tp = cm[i].ravel()\n",
        "    total_tn += tn\n",
        "    total_fp += fp\n",
        "    total_fn += fn\n",
        "    total_tp += tp\n",
        "\n",
        "# Create the combined confusion matrix\n",
        "combined_cm = np.array([\n",
        "    [total_tn, total_fp],\n",
        "    [total_fn, total_tp]\n",
        "])\n",
        "\n",
        "# Plot the combined confusion matrix\n",
        "fig = plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(combined_cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted No', 'Predicted Yes'],\n",
        "            yticklabels=['Actual No', 'Actual Yes'])\n",
        "plt.title('Combined Confusion Matrix for All Tags')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}